behaviors:
  RagdollAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024 # Reduced to match cube config to prevent timeouts
      buffer_size: 10240 # Reduced to match cube config for better responsiveness
      learning_rate: 3.0e-4 # Standard learning rate
      beta: 0.01 # Slightly higher entropy for exploration (was 0.008)
      epsilon: 0.2 # Standard PPO clipping
      lambd: 0.95 # Standard GAE parameter
      num_epoch: 3 # Reduced from 4 to match cube config (faster training)
    network_settings:
      normalize: true # Essential for ragdoll observations
      hidden_units: 128 # Reduced to match cube config for faster training
      num_layers: 2 # Reduced to match cube config to prevent timeouts
      activation: relu # Explicit activation function
    max_steps: 300000 # Further reduced to prevent timeouts
    time_horizon: 256 # Much shorter episodes to prevent Unity hangs
    summary_freq: 500 # Very frequent summaries for quick feedback
    keep_checkpoints: 10 # Keep more checkpoints for safety
    checkpoint_interval: 25000 # Very frequent checkpoints to prevent loss
    threaded: true # Enable threading for performance
    reward_signals:
      extrinsic:
        gamma: 0.99 # Standard discount factor
        strength: 1.0
