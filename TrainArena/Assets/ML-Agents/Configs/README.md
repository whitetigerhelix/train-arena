# ML-Agents Training Configuration Guide

This folder contains YAML configuration files that control how ML-Agents trains your reinforcement learning agents. Understanding these parameters is key to successful training.

## üìã **Current Configurations**

- **`cube_ppo.yaml`** - Main configuration for CubeAgent (reach goal, avoid obstacles)
- **`ragdoll_ppo.yaml`** - Configuration for ragdoll locomotion training
- **`runner_ppo.yaml`** - Alternative running/movement configuration
- **`selfplay_*.yaml`** - Self-play configurations for competitive training

---

## üß† **PPO Algorithm Basics**

**Proximal Policy Optimization (PPO)** is the default algorithm in ML-Agents. It's:

- **Stable**: Less likely to catastrophically forget what it learned
- **Sample efficient**: Makes good use of collected experience
- **Robust**: Works well across many different types of problems

PPO works by:

1. **Collecting experience** by running the agent in the environment
2. **Learning from batches** of that experience multiple times
3. **Updating the policy** carefully to avoid large changes

---

## ‚öôÔ∏è **Configuration Breakdown: cube_ppo.yaml**

### **Core Training Settings**

```yaml
behaviors:
  CubeAgent: # Must match the Behavior Name in Unity
    trainer_type: ppo # Algorithm: ppo, sac, or poca
    max_steps: 500000 # Total training steps before stopping
    time_horizon: 64 # Steps per episode before truncation
    summary_freq: 5000 # How often to log training metrics
```

**Our Analysis:**

- ‚úÖ **500K steps**: Good for initial training (faster results)
- ‚úÖ **64 time horizon**: Reasonable for cube navigation task
- ‚úÖ **5K summary frequency**: Good balance of detail vs performance

### **Hyperparameters Section**

```yaml
hyperparameters:
  batch_size: 1024 # Training batch size
  buffer_size: 10240 # Experience replay buffer size
  learning_rate: 3.0e-4 # How fast the agent learns (0.0003)
  beta: 5.0e-3 # Entropy regularization (0.005)
  epsilon: 0.2 # PPO clipping parameter
  lambd: 0.95 # GAE lambda (advantage estimation)
  num_epoch: 3 # Training epochs per update
```

**Parameter Analysis:**

#### **batch_size: 1024** ‚öñÔ∏è _Good Balance_

- **What it does**: How many experiences to learn from at once
- **Trade-offs**:
  - Higher = more stable learning, slower training
  - Lower = faster training, potentially unstable
- **Our choice**: 1024 is excellent for cube tasks - stable but not slow

#### **buffer_size: 10240** ‚öñÔ∏è _Conservative_

- **What it does**: How much experience to collect before training
- **Rule of thumb**: Usually 10x batch_size (we have 10x, perfect!)
- **Trade-offs**:
  - Higher = more diverse experience, better stability
  - Lower = faster updates, less memory usage
- **Our choice**: Conservative but good for learning stability

#### **learning_rate: 3.0e-4** ‚úÖ _Excellent Default_

- **What it does**: How big steps the agent takes when learning
- **Sweet spot**: 1e-4 to 1e-3 for most tasks
- **Our choice**: Right in the sweet spot - proven to work well

#### **beta: 5.0e-3** ‚úÖ _Good Exploration_

- **What it does**: Encourages exploration vs exploitation
- **Higher values**: More random exploration
- **Lower values**: More focused on current best strategy
- **Our choice**: Moderate exploration - good for learning phase

#### **epsilon: 0.2** ‚úÖ _PPO Standard_

- **What it does**: Prevents the policy from changing too drastically
- **Standard range**: 0.1 to 0.3
- **Our choice**: Perfect default - stable learning

#### **lambd: 0.95** ‚úÖ _Standard_

- **What it does**: How much to trust future rewards vs immediate ones
- **Our choice**: Standard value, works well for most tasks

#### **num_epoch: 3** ‚úÖ _Conservative_

- **What it does**: How many times to train on each batch of experience
- **Trade-offs**:
  - Higher = more learning per experience (but risk overfitting)
  - Lower = less overfitting (but might waste good experience)
- **Our choice**: Conservative - prevents overfitting in early training

### **Network Settings**

```yaml
network_settings:
  normalize: true # Input normalization
  hidden_units: 128 # Neural network size
  num_layers: 2 # Neural network depth
```

**Analysis:**

#### **normalize: true** ‚úÖ _Optimized_

- **What it does**: Normalizes input observations to standard ranges
- **Benefits**: Helps neural network learn faster and more stably
- **Our choice**: Enabled - provides faster convergence and smoother learning curves
- **Impact**: Expected 20-50% improvement in learning speed for mixed observation types

#### **hidden_units: 128** ‚úÖ _Good Size_

- **What it does**: Size of each neural network layer
- **Rule of thumb**:
  - Simple tasks: 64-128 units
  - Complex tasks: 256-512 units
- **Our choice**: Perfect for cube navigation task

#### **num_layers: 2** ‚úÖ _Appropriate Depth_

- **What it does**: How many hidden layers in the network
- **Trade-offs**:
  - More layers = can learn more complex behaviors
  - Fewer layers = faster training, less overfitting risk
- **Our choice**: Good for cube task complexity

### **Reward Signals**

```yaml
reward_signals:
  extrinsic:
    gamma: 0.99 # Discount factor for future rewards
    strength: 1.0 # Reward signal weight
```

**Analysis:**

#### **gamma: 0.99** ‚úÖ _Forward-Looking_

- **What it does**: How much the agent cares about future rewards
- **0.9**: Focuses on immediate rewards
- **0.99**: Considers long-term consequences
- **0.999**: Very long-term planning
- **Our choice**: Great for navigation tasks requiring planning

#### **strength: 1.0** ‚úÖ _Standard_

- **What it does**: Multiplier for extrinsic (environment) rewards
- **Our choice**: Standard - uses rewards as designed

---

## üéØ **Is Our Configuration Good?**

### **‚úÖ Strengths**

1. **Optimized for stability**: Input normalization + conservative settings prevent instability
2. **Proven parameters**: Uses well-tested defaults for most values
3. **Appropriate complexity**: Network size matches task complexity
4. **Good exploration**: Beta value encourages learning new behaviors
5. **Fast convergence**: Normalization enables 20-50% faster learning

### **‚ö†Ô∏è Potential Improvements**

1. **Consider longer training**: 500K steps might be short for complex behaviors
2. **Larger buffer**: Could increase to 20480 for more diverse experience
3. **Higher learning rate**: Could try 5e-4 for faster convergence

### **üöÄ Current Optimized Configuration**

```yaml
# For longer training runs
max_steps: 1000000 # Double training length
buffer_size: 20480 # More diverse experience
```

---

## üìä **Performance Tuning Guide**

### **If Training is Too Slow:**

- ‚Üì Reduce `buffer_size` (e.g., 5120)
- ‚Üì Reduce `batch_size` (e.g., 512)
- ‚Üì Reduce `num_epoch` (e.g., 2)

### **If Learning is Unstable:**

- ‚Üë Increase `buffer_size` (e.g., 20480)
- ‚Üì Reduce `learning_rate` (e.g., 1e-4)
- ‚úÖ Normalization already enabled for stability

### **If Agent Gets Stuck in Local Optima:**

- ‚Üë Increase `beta` (e.g., 1e-2) for more exploration
- ‚Üë Increase `epsilon` (e.g., 0.3) for more policy variation
- ‚Üë Increase `buffer_size` for more diverse experience

### **If Training Takes Too Long:**

- ‚Üë Increase `learning_rate` (e.g., 5e-4)
- ‚Üë Increase `num_epoch` (e.g., 4-5)
- ‚Üë Increase `batch_size` (e.g., 2048)

---

## üî¨ **Experiment Tracking**

### **Key Metrics to Watch in TensorBoard:**

1. **Cumulative Reward**: Should generally increase over time
2. **Episode Length**: How long episodes last before termination
3. **Policy Loss**: Should decrease and stabilize
4. **Value Loss**: Network's ability to predict rewards
5. **Entropy**: Measure of exploration (should start high, then decrease)

### **Good Training Signs:**

- ‚úÖ Reward increases over time (with some noise)
- ‚úÖ Policy loss decreases and stabilizes
- ‚úÖ Episode length increases (agent survives longer)
- ‚úÖ Entropy starts high then gradually decreases

### **Bad Training Signs:**

- ‚ùå Reward stays flat or decreases
- ‚ùå Policy loss increases or oscillates wildly
- ‚ùå Episode length stays very short
- ‚ùå Entropy drops to near zero too quickly

---

## üìö **Further Learning Resources**

- **ML-Agents Documentation**: https://unity-technologies.github.io/ml-agents/
- **PPO Paper**: "Proximal Policy Optimization Algorithms" (Schulman et al.)
- **Hyperparameter Tuning**: Start with our config, then experiment with one parameter at a time
- **Unity Learn**: ML-Agents courses and tutorials

---

## üéÆ **Quick Start Summary**

1. **Start with our config** - it's well-balanced for learning
2. **Train for 500K steps** - check TensorBoard for progress
3. **If results are good**: Increase `max_steps` to 1-2M for final training
4. **If learning is slow**: Increase `learning_rate` to 5e-4
5. **If need more stability**: Increase `buffer_size` to 20480 (and `normalize: true` if not enabled)

**Happy Training!** üöÄ

The key to successful RL is patience and experimentation. Start with stable settings, then optimize based on your specific results!
